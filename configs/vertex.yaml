seed_everything: 2022

trainer:
  callbacks:
    - class_path: pytorch_lightning.callbacks.RichProgressBar
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        monitor: val/fmeasure
        mode: max
        filename: checkpoint_{epoch:03d}-precision={val/precision:.3f}-recall={val/recall:.3f}-f1={val/fmeasure:.3f}
        auto_insert_metric_name: False
        verbose: True
        save_top_k: 1
        save_last: True
  benchmark: True
  detect_anomaly: True
  num_sanity_val_steps: 0
  strategy: ddp
  check_val_every_n_epoch: 20
  accumulate_grad_batches: 2
  devices: 4
  accelerator: gpu
  max_epochs: 1000

model:
  hparams:
    root: data/data/polygen/infos
    datasets_train: data/splits/train.txt
    datasets_valid: data/splits/valid.txt
    datasets_test: data/splits/test.txt
    batch_size: 8
    num_workers: 4
    lr: 1e-4

    # data
    data:
      num_line_dof: 4
      max_line_seq: 1200
      max_vert_seq: 480

      quantization_bits: 9

      aug_ratio: 0.1
      noise_ratio: 0.15
      noise_length: 0.02

    token:
      END: 512
      PAD: 513

    model:
      num_model: 512
      num_head: 8
      num_feedforward: 1024
      dropout: 0.2
      activation: relu
      normalize_before: True
      num_encoder_layers: 6
      num_decoder_layers: 6
      vocab_size: 514
      num_type: 2
      num_view: 3
